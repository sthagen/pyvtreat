{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Split Cross-Validation Data Leak\n",
    "\n",
    "In our article [Cross-Methods are a Leak/Variance Trade-Off](https://github.com/WinVector/pyvtreat/blob/master/Examples/CrossVal/LeakTradeOff/CrossFrameExample.ipynb) we left out an interesting data leak: what happens if  you use different cross-validation plans on each step of a multi-stage or multi column problem?  \n",
    "\n",
    "We thought of this example: because our students have asked us this very question. We have found our students are very smart and their questions often are very fundamental.  So here is a concrete example of some of the risk using many different cross-validation plans in the same project: each leaks different information, which can be combined into a larger leak.  The example here is extreme, but it can make the point.\n",
    "\n",
    "So our advice is: unless you are coordinating the many plans in some way (such as 2-way independence or some sort of combinatorial design) it is generally better to use one plan. That way minor information leaks at each stage explore less of the output variations, and donâ€™t combine into worse leaks.\n",
    "\n",
    "Lets take a look at what having a lot of copies of the constant column encoded under different cross validation plans looks like.  For this example we will use 3-fold cross validation.  That leans less, but as we see averaging a lot of copies of the constant column treated in the same way still leaks information in a non-productive manner.  For a 3-fold cross-validation plan the bias from any one column is small, however many columns together will leak information.  We will demonstrate this next.\n",
    "\n",
    "First we import our required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://numpy.org\n",
    "import numpy\n",
    "\n",
    "# https://pandas.pydata.org\n",
    "import pandas\n",
    "\n",
    "# https://scikit-learn.org/\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "# https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html\n",
    "import category_encoders\n",
    "\n",
    "# https://www.statsmodels.org/\n",
    "import statsmodels.api\n",
    "\n",
    "# https://github.com/WinVector/pyvtreat/blob/master/Examples/CrossVal/LeakTradeOff/break_cross_val.py\n",
    "from break_cross_val import TransformerAdapter, Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a data frame of all the same constant value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(2020)\n",
    "prng = numpy.random.RandomState(numpy.random.randint(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_example = numpy.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2  3  4  5  6  7  8  9  ... 990 991 992 993 994 995 996 997 998 999\n",
       "0   a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "1   a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "2   a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "3   a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "4   a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       ".. .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "95  a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "96  a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "97  a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "98  a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "99  a  a  a  a  a  a  a  a  a  a  ...   a   a   a   a   a   a   a   a   a   a\n",
       "\n",
       "[100 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_const_frame = pandas.DataFrame()\n",
    "for j in range(1000):\n",
    "    wide_const_frame[str(j)] = ['a'] * len(y_example)\n",
    "\n",
    "wide_const_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a different cross-validation plan for each column/step (not a good idea)\n",
    "\n",
    "\n",
    "Now we re-code the frame using a different cross-validation plan for each column (an inadvisable idea).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.123506</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.155730</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>-0.035786</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.216629</td>\n",
       "      <td>0.079401</td>\n",
       "      <td>0.172212</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>0.095523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.056868</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.155730</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>0.153948</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.098883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.198822</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>0.159744</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.172212</td>\n",
       "      <td>0.102955</td>\n",
       "      <td>0.095523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.104923</td>\n",
       "      <td>0.123506</td>\n",
       "      <td>0.115078</td>\n",
       "      <td>0.155730</td>\n",
       "      <td>0.170804</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>0.264864</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.198822</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>-0.087162</td>\n",
       "      <td>0.199564</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.048475</td>\n",
       "      <td>0.038054</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>0.027020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.101311</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>0.170804</td>\n",
       "      <td>0.153948</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.157165</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.199564</td>\n",
       "      <td>0.216629</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.172212</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>0.027020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.101311</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.115078</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>-0.035786</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.157165</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.048475</td>\n",
       "      <td>0.172212</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>0.027020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>0.264864</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.199564</td>\n",
       "      <td>0.043185</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.038054</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>0.145680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.104923</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.079868</td>\n",
       "      <td>0.106253</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>0.264864</td>\n",
       "      <td>-0.035786</td>\n",
       "      <td>0.098883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181426</td>\n",
       "      <td>0.198822</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.216629</td>\n",
       "      <td>0.048475</td>\n",
       "      <td>0.038054</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>0.145680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.101311</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.106253</td>\n",
       "      <td>0.170804</td>\n",
       "      <td>0.153948</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.098883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181426</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.199564</td>\n",
       "      <td>0.216629</td>\n",
       "      <td>0.079401</td>\n",
       "      <td>0.172212</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>0.095523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>0.079868</td>\n",
       "      <td>0.106253</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>0.098883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.157165</td>\n",
       "      <td>-0.087162</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.043185</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.058426</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>0.027020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.055096</td>\n",
       "      <td>0.101311</td>\n",
       "      <td>0.123506</td>\n",
       "      <td>0.079868</td>\n",
       "      <td>0.155730</td>\n",
       "      <td>0.170804</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.093048</td>\n",
       "      <td>0.152003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181426</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.157165</td>\n",
       "      <td>0.159744</td>\n",
       "      <td>0.199564</td>\n",
       "      <td>0.216629</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.058426</td>\n",
       "      <td>0.102955</td>\n",
       "      <td>0.145680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.055096  0.063151  0.123506  0.073977  0.155730  0.125445  0.118004   \n",
       "1   0.053041  0.063151  0.056868  0.073977  0.155730  0.125445  0.153948   \n",
       "2   0.055096  0.104923  0.123506  0.115078  0.155730  0.170804 -0.002372   \n",
       "3   0.160505  0.101311  0.089289  0.073977  0.005925  0.170804  0.153948   \n",
       "4   0.160505  0.101311  0.089289  0.115078  0.005925  0.125445  0.118004   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.160505  0.063151  0.089289  0.073977  0.005925 -0.026558 -0.002372   \n",
       "96  0.053041  0.104923  0.089289  0.079868  0.106253  0.125445  0.118004   \n",
       "97  0.160505  0.101311  0.089289  0.073977  0.106253  0.170804  0.153948   \n",
       "98  0.055096  0.063151  0.089289  0.079868  0.106253 -0.026558  0.118004   \n",
       "99  0.055096  0.101311  0.123506  0.079868  0.155730  0.170804  0.118004   \n",
       "\n",
       "           7         8         9  ...       990       991       992       993  \\\n",
       "0  -0.011715 -0.035786  0.152003  ...  0.054028  0.024162  0.089872  0.197621   \n",
       "1   0.018624  0.210023  0.098883  ...  0.032855  0.198822  0.022123  0.159744   \n",
       "2   0.264864  0.210023  0.019202  ...  0.032855  0.198822  0.022123 -0.087162   \n",
       "3   0.018624  0.210023  0.152003  ...  0.054028  0.024162  0.157165  0.197621   \n",
       "4   0.018624 -0.035786  0.019202  ...  0.054028  0.024162  0.157165  0.197621   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.264864  0.210023  0.019202  ...  0.032855  0.024162  0.089872  0.197621   \n",
       "96  0.264864 -0.035786  0.098883  ...  0.181426  0.198822  0.089872  0.197621   \n",
       "97 -0.011715  0.210023  0.098883  ...  0.181426  0.045514  0.089872  0.197621   \n",
       "98  0.018624  0.210023  0.098883  ...  0.032855  0.045514  0.157165 -0.087162   \n",
       "99  0.018624  0.093048  0.152003  ...  0.181426  0.045514  0.157165  0.159744   \n",
       "\n",
       "         994       995       996       997       998       999  \n",
       "0   0.012531  0.216629  0.079401  0.172212  0.091442  0.095523  \n",
       "1   0.012531  0.008650  0.141128  0.172212  0.102955  0.095523  \n",
       "2   0.199564  0.008650  0.048475  0.038054  0.074787  0.027020  \n",
       "3   0.199564  0.216629  0.141128  0.172212  0.091442  0.027020  \n",
       "4   0.012531  0.008650  0.048475  0.172212  0.091442  0.027020  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.199564  0.043185  0.141128  0.038054  0.074787  0.145680  \n",
       "96  0.056569  0.216629  0.048475  0.038054  0.091442  0.145680  \n",
       "97  0.199564  0.216629  0.079401  0.172212  0.074787  0.095523  \n",
       "98  0.012531  0.043185  0.141128  0.058426  0.091442  0.027020  \n",
       "99  0.199564  0.216629  0.141128  0.058426  0.102955  0.145680  \n",
       "\n",
       "[100 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross encode each column with a different plan \n",
    "# (not a good idea)\n",
    "wide_coded_frame = pandas.DataFrame()\n",
    "\n",
    "for c in wide_const_frame.columns:\n",
    "    # http://www.win-vector.com/blog/2020/03/python-data-science-tip-dont-use-default-cross-validation-settings/\n",
    "    cvstrat = sklearn.model_selection.KFold(\n",
    "        shuffle=True, n_splits=3,\n",
    "        random_state=prng)\n",
    "    te_wide = TransformerAdapter(category_encoders.target_encoder.TargetEncoder())\n",
    "    colf = sklearn.model_selection.cross_val_predict(\n",
    "        te_wide, \n",
    "        wide_const_frame[[c]], \n",
    "        y_example, \n",
    "        cv=cvstrat)\n",
    "    wide_coded_frame[c] = colf[:, 0]\n",
    "\n",
    "wide_coded_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Single column is uninformative\n",
    "\n",
    "The cross validation is useful to the degree that any single column is uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 14 Mar 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.115</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:23:42</td>     <th>  Log-Likelihood:    </th> <td> -147.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   299.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   304.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>     <td>   -2.0249</td> <td>    1.272</td> <td>   -1.591</td> <td> 0.115</td> <td>   -4.550</td> <td>    0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.2725</td> <td>    0.157</td> <td>    1.736</td> <td> 0.086</td> <td>   -0.039</td> <td>    0.584</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.921</td> <th>  Durbin-Watson:     </th> <td>   2.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.631</td> <th>  Jarque-Bera (JB):  </th> <td>   0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.004</td> <th>  Prob(JB):          </th> <td>   0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.327</td> <th>  Cond. No.          </th> <td>    12.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.025\n",
       "Model:                            OLS   Adj. R-squared:                  0.015\n",
       "Method:                 Least Squares   F-statistic:                     2.533\n",
       "Date:                Sat, 14 Mar 2020   Prob (F-statistic):              0.115\n",
       "Time:                        12:23:42   Log-Likelihood:                -147.60\n",
       "No. Observations:                 100   AIC:                             299.2\n",
       "Df Residuals:                      98   BIC:                             304.4\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x             -2.0249      1.272     -1.591      0.115      -4.550       0.500\n",
       "const          0.2725      0.157      1.736      0.086      -0.039       0.584\n",
       "==============================================================================\n",
       "Omnibus:                        0.921   Durbin-Watson:                   2.040\n",
       "Prob(Omnibus):                  0.631   Jarque-Bera (JB):                0.447\n",
       "Skew:                           0.004   Prob(JB):                        0.800\n",
       "Kurtosis:                       3.327   Cond. No.                         12.0\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c5_frame = pandas.DataFrame({\n",
    "    'x': wide_coded_frame['5'], \n",
    "    'const': 1})\n",
    "\n",
    "fit_c5 = statsmodels.api.OLS(\n",
    "    y_example, \n",
    "    c5_frame)\n",
    "res_c5 = fit_c5.fit()\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=res_c5.predict(c5_frame))\n",
    "\n",
    "assert r2 < 0.1\n",
    "\n",
    "res_c5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Mean encoding\n",
    "\n",
    "The mean of these encodings has information every row nearly uniformly, except the row itself. This is much like the leave-one out pattern, and can is a data leak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_noise</th>\n",
       "      <th>const_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.090376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.095393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.111182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.058898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.116958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_noise  const_col\n",
       "0     0.091302          1\n",
       "1     0.099572          1\n",
       "2     0.089368          1\n",
       "3     0.103004          1\n",
       "4     0.070237          1\n",
       "..         ...        ...\n",
       "95    0.090376          1\n",
       "96    0.095393          1\n",
       "97    0.111182          1\n",
       "98    0.058898          1\n",
       "99    0.116958          1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_coded_noise = wide_coded_frame.mean(axis = 1) \n",
    "\n",
    "wide_fit = pandas.DataFrame({\n",
    "    'mean_noise': mean_coded_noise,\n",
    "    'const_col': 1\n",
    "})\n",
    "\n",
    "wide_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leak is demonstrated by showing the average column can be used estimate the explanatory variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1925.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 14 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>3.08e-66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:23:42</td>     <th>  Log-Likelihood:    </th> <td>  2.5024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -1.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   4.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_noise</th> <td>  -95.9396</td> <td>    2.186</td> <td>  -43.878</td> <td> 0.000</td> <td> -100.279</td> <td>  -91.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const_col</th>  <td>    8.6952</td> <td>    0.198</td> <td>   44.012</td> <td> 0.000</td> <td>    8.303</td> <td>    9.087</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.031</td> <th>  Durbin-Watson:     </th> <td>   1.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.597</td> <th>  Jarque-Bera (JB):  </th> <td>   1.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.116</td> <th>  Prob(JB):          </th> <td>   0.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.557</td> <th>  Cond. No.          </th> <td>    92.5</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.952\n",
       "Model:                            OLS   Adj. R-squared:                  0.951\n",
       "Method:                 Least Squares   F-statistic:                     1925.\n",
       "Date:                Sat, 14 Mar 2020   Prob (F-statistic):           3.08e-66\n",
       "Time:                        12:23:42   Log-Likelihood:                 2.5024\n",
       "No. Observations:                 100   AIC:                            -1.005\n",
       "Df Residuals:                      98   BIC:                             4.206\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "mean_noise   -95.9396      2.186    -43.878      0.000    -100.279     -91.601\n",
       "const_col      8.6952      0.198     44.012      0.000       8.303       9.087\n",
       "==============================================================================\n",
       "Omnibus:                        1.031   Durbin-Watson:                   1.861\n",
       "Prob(Omnibus):                  0.597   Jarque-Bera (JB):                1.039\n",
       "Skew:                          -0.116   Prob(JB):                        0.595\n",
       "Kurtosis:                       2.557   Cond. No.                         92.5\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_model_wide = statsmodels.api.OLS(\n",
    "    y_example, \n",
    "    wide_fit)\n",
    "overfit_result_wide = overfit_model_wide.fit()\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=overfit_result_wide.predict(wide_fit))\n",
    "\n",
    "assert r2 > 0.8\n",
    "\n",
    "overfit_result_wide.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the data leak essentially non-productively memorized the outcome.  Again, the issue is: this method won't work on out of sample data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the same cross-validation plan for each column/step (the recommendation)\n",
    "\n",
    "Now let's see how this works when we use the more standard recommended method of using the same cross-validation plan throughout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>0.124471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.118434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471   \n",
       "1   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682   \n",
       "2   0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471   \n",
       "3   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682   \n",
       "4   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471   \n",
       "96  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682   \n",
       "97  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471   \n",
       "98  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434   \n",
       "99  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434   \n",
       "\n",
       "           7         8         9  ...       990       991       992       993  \\\n",
       "0   0.124471  0.124471  0.124471  ...  0.124471  0.124471  0.124471  0.124471   \n",
       "1   0.026682  0.026682  0.026682  ...  0.026682  0.026682  0.026682  0.026682   \n",
       "2   0.124471  0.124471  0.124471  ...  0.124471  0.124471  0.124471  0.124471   \n",
       "3   0.026682  0.026682  0.026682  ...  0.026682  0.026682  0.026682  0.026682   \n",
       "4   0.026682  0.026682  0.026682  ...  0.026682  0.026682  0.026682  0.026682   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.124471  0.124471  0.124471  ...  0.124471  0.124471  0.124471  0.124471   \n",
       "96  0.026682  0.026682  0.026682  ...  0.026682  0.026682  0.026682  0.026682   \n",
       "97  0.124471  0.124471  0.124471  ...  0.124471  0.124471  0.124471  0.124471   \n",
       "98  0.118434  0.118434  0.118434  ...  0.118434  0.118434  0.118434  0.118434   \n",
       "99  0.118434  0.118434  0.118434  ...  0.118434  0.118434  0.118434  0.118434   \n",
       "\n",
       "         994       995       996       997       998       999  \n",
       "0   0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  \n",
       "1   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  \n",
       "2   0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  \n",
       "3   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  \n",
       "4   0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  \n",
       "96  0.026682  0.026682  0.026682  0.026682  0.026682  0.026682  \n",
       "97  0.124471  0.124471  0.124471  0.124471  0.124471  0.124471  \n",
       "98  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434  \n",
       "99  0.118434  0.118434  0.118434  0.118434  0.118434  0.118434  \n",
       "\n",
       "[100 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross encode each column with the same plan \n",
    "# (the standard advice)\n",
    "class PreStoredCrossVal:\n",
    "    def __init__(self, plan, nrow):\n",
    "        self.n_splits = plan.get_n_splits()\n",
    "        self.nrow = nrow\n",
    "        self.plan = [(train, test) for train, test in plan.split(pandas.DataFrame({'x': range(nrow)}))]\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if X.shape[0] != self.nrow:\n",
    "            raise ValueError(\"number of rows must be \" + str(self.nrow))\n",
    "        return [(train, test) for train, test in self.plan]\n",
    "\n",
    "    \n",
    "cvstratc = PreStoredCrossVal(\n",
    "    sklearn.model_selection.KFold(\n",
    "        shuffle=True, n_splits=3,\n",
    "        random_state=prng),\n",
    "    nrow=wide_const_frame.shape[0])\n",
    "\n",
    "\n",
    "wide_coded_framec = pandas.DataFrame()\n",
    "\n",
    "\n",
    "for c in wide_const_frame.columns:\n",
    "    # http://www.win-vector.com/blog/2020/03/python-data-science-tip-dont-use-default-cross-validation-settings/\n",
    "    te_wide = TransformerAdapter(category_encoders.target_encoder.TargetEncoder())\n",
    "    colf = sklearn.model_selection.cross_val_predict(\n",
    "        te_wide, \n",
    "        wide_const_frame[[c]], \n",
    "        y_example, \n",
    "        cv=cvstratc)\n",
    "    wide_coded_framec[c] = colf[:, 0]\n",
    "\n",
    "wide_coded_framec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all of the columns are identical, so using any one of them is the same as using all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.6956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 14 Mar 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.406</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:25:05</td>     <th>  Log-Likelihood:    </th> <td> -148.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   301.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   306.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>     <td>   -2.0179</td> <td>    2.420</td> <td>   -0.834</td> <td> 0.406</td> <td>   -6.820</td> <td>    2.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.2716</td> <td>    0.243</td> <td>    1.116</td> <td> 0.267</td> <td>   -0.211</td> <td>    0.755</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.881</td> <th>  Durbin-Watson:     </th> <td>   2.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.390</td> <th>  Jarque-Bera (JB):  </th> <td>   1.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.040</td> <th>  Prob(JB):          </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.580</td> <th>  Cond. No.          </th> <td>    22.6</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.007\n",
       "Model:                            OLS   Adj. R-squared:                 -0.003\n",
       "Method:                 Least Squares   F-statistic:                    0.6956\n",
       "Date:                Sat, 14 Mar 2020   Prob (F-statistic):              0.406\n",
       "Time:                        12:25:05   Log-Likelihood:                -148.52\n",
       "No. Observations:                 100   AIC:                             301.0\n",
       "Df Residuals:                      98   BIC:                             306.3\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x             -2.0179      2.420     -0.834      0.406      -6.820       2.784\n",
       "const          0.2716      0.243      1.116      0.267      -0.211       0.755\n",
       "==============================================================================\n",
       "Omnibus:                        1.881   Durbin-Watson:                   2.002\n",
       "Prob(Omnibus):                  0.390   Jarque-Bera (JB):                1.427\n",
       "Skew:                          -0.040   Prob(JB):                        0.490\n",
       "Kurtosis:                       3.580   Cond. No.                         22.6\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_frame = pandas.DataFrame({\n",
    "    'x': wide_coded_framec['0'], \n",
    "    'const': 1})\n",
    "\n",
    "fit_c = statsmodels.api.OLS(\n",
    "    y_example, \n",
    "    c_frame)\n",
    "res_c = fit_c.fit()\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(\n",
    "    y_true=y_example, \n",
    "    y_pred=res_c.predict(c_frame))\n",
    "\n",
    "assert r2 < 0.1\n",
    "\n",
    "res_c.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `TransformerAdapter`\n",
    "\n",
    "As a side note, the reason we are using `TransformerAdapter` is `category_encoders.target_encoder.TargetEncoder` doesn't (at version `2.1.0`) implement all of the interface needed for `sklearn.model_selection.cross_val_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_encoders.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TargetEncoder' object has no attribute 'predict'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    colf = sklearn.model_selection.cross_val_predict(\n",
    "        category_encoders.target_encoder.TargetEncoder(), \n",
    "        wide_const_frame[[c]], \n",
    "        y_example, \n",
    "        cv=cvstratc)\n",
    "except AttributeError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this time a single column being uninformative means the columns are uninformative jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And here we have a fairly simple demonstration of a data leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
