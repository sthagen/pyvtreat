{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the concept of \"regression to mediocrity\" as described in Nina Zumel's great article [*Why Do We Plot Predictions on the x-axis?*](http://www.win-vector.com/blog/2019/09/why-do-we-plot-predictions-on-the-x-axis/).\n",
    "\n",
    "This time let's consider the issue from the point of view of multinomial classification (a concept discussed [here](https://github.com/WinVector/pyvtreat/blob/master/Examples/Multinomial/MultinomialExample.md)).\n",
    "\n",
    "First we load our packages and generate some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy.random\n",
    "import pandas\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.389409</td>\n",
       "      <td>-2.115627</td>\n",
       "      <td>indeterminate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.354096</td>\n",
       "      <td>-0.195495</td>\n",
       "      <td>indeterminate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.057603</td>\n",
       "      <td>0.928929</td>\n",
       "      <td>indeterminate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.400339</td>\n",
       "      <td>-0.936919</td>\n",
       "      <td>indeterminate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.125245</td>\n",
       "      <td>-0.220789</td>\n",
       "      <td>indeterminate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2              y\n",
       "0  0.389409 -2.115627  indeterminate\n",
       "1 -0.354096 -0.195495  indeterminate\n",
       "2 -0.057603  0.928929  indeterminate\n",
       "3 -0.400339 -0.936919  indeterminate\n",
       "4 -0.125245 -0.220789  indeterminate"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(34524)\n",
    "\n",
    "N = 1000\n",
    "\n",
    "df = pandas.DataFrame({\n",
    "    'x1': numpy.random.normal(size=N),\n",
    "    'x2': numpy.random.normal(size=N),\n",
    "    })\n",
    "noise = numpy.random.normal(size=N)\n",
    "y = df.x1 + df.x2 + noise\n",
    "df['y'] = numpy.where(\n",
    "    y < -3, \n",
    "    'short_opportunity', \n",
    "    numpy.where(\n",
    "        y > 3, \n",
    "        'long_opportunity', \n",
    "        'indeterminate'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indeterminate        925\n",
       "short_opportunity     41\n",
       "long_opportunity      34\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pretend this data is a record of stock market trading situations where we have determined (by peaking into the future, something quite easy to do with historic data) there is a large opportunity to make money buying security (called `long_opportunity`) or a larger opportunity to make money selling a security (called `short_opportunity`).\n",
    "\n",
    "Let's build a model using the two observable dependent variables `x1` and `x2`.  These are measurements that are available at the time of the proposed trade that we hope correlate with or \"predict\" the future trading result.  For our model we will use a simple multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vars = ['x1', 'x2']\n",
    "\n",
    "fitter = sklearn.linear_model.LogisticRegression(\n",
    "    solver = 'saga',\n",
    "    penalty = 'l2',\n",
    "    C = 1,\n",
    "    max_iter = 1e+5,\n",
    "    multi_class = 'multinomial')\n",
    "fitter.fit(df[model_vars], df['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then examining the model predictions on the training data itself (a *much* lower standard than evaluating the model on held out data!!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convenience functions for predicting and adding predictions to original data frame\n",
    "\n",
    "def add_predictions(d_prepared, model_vars, fitter):\n",
    "    pred = fitter.predict_proba(d_prepared[model_vars])\n",
    "    classes = fitter.classes_\n",
    "    d_prepared['prob_on_predicted_class'] = 0\n",
    "    d_prepared['prediction'] = None\n",
    "    for i in range(len(classes)):\n",
    "        cl = classes[i]\n",
    "        d_prepared[cl] = pred[:, i]\n",
    "        improved = d_prepared[cl] > d_prepared['prob_on_predicted_class']\n",
    "        d_prepared.loc[improved, 'prediction'] = cl\n",
    "        d_prepared.loc[improved, 'prob_on_predicted_class'] = d_prepared.loc[improved, cl]\n",
    "    return d_prepared\n",
    "\n",
    "def add_value_by_column(d_prepared, name_column, new_column):\n",
    "    vals = d_prepared[name_column].unique()\n",
    "    d_prepared[new_column] = None\n",
    "    for v in vals:\n",
    "        matches = d_prepared[name_column]==v\n",
    "        d_prepared.loc[matches, new_column] = d_prepared.loc[matches, v]\n",
    "    return d_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df['prediction'] = fitter.predict(df[model_vars])\n",
    "df = add_predictions(df, model_vars, fitter)\n",
    "df = add_value_by_column(df, 'y', 'prob_on_correct_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>prob_on_predicted_class</th>\n",
       "      <th>prediction</th>\n",
       "      <th>indeterminate</th>\n",
       "      <th>long_opportunity</th>\n",
       "      <th>short_opportunity</th>\n",
       "      <th>prob_on_correct_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.949149</td>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.949149</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.949149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.989852</td>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.989852</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.989852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.982227</td>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.982227</td>\n",
       "      <td>0.017159</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.982227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.964236</td>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.964236</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>0.964236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.992411</td>\n",
       "      <td>indeterminate</td>\n",
       "      <td>0.992411</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.992411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               y  prob_on_predicted_class     prediction  indeterminate  \\\n",
       "0  indeterminate                 0.949149  indeterminate       0.949149   \n",
       "1  indeterminate                 0.989852  indeterminate       0.989852   \n",
       "2  indeterminate                 0.982227  indeterminate       0.982227   \n",
       "3  indeterminate                 0.964236  indeterminate       0.964236   \n",
       "4  indeterminate                 0.992411  indeterminate       0.992411   \n",
       "\n",
       "   long_opportunity  short_opportunity prob_on_correct_class  \n",
       "0          0.000175           0.050676              0.949149  \n",
       "1          0.001375           0.008773              0.989852  \n",
       "2          0.017159           0.000614              0.982227  \n",
       "3          0.000332           0.035432              0.964236  \n",
       "4          0.002010           0.005579              0.992411  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_columns = ['y', 'prob_on_predicted_class', 'prediction', \n",
    "                  'indeterminate', 'long_opportunity', \n",
    "                  'short_opportunity', 'prob_on_correct_class']\n",
    "df[result_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, as described in [*The Simpler Derivation of Logistic Regression*](http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/) that the sums of the prediction probabilities essentially equal the counts of each category on the training data (differences due to numeric issues and regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_opportunity     41.007198\n",
       "indeterminate        924.988576\n",
       "long_opportunity      34.004226\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['short_opportunity', 'indeterminate', 'long_opportunity']].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indeterminate        925\n",
       "short_opportunity     41\n",
       "long_opportunity      34\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to examine the relation of the model predictions to outcomes is a graphical table called a *confusion matrix*.  The [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) has states:\n",
    "\n",
    "> By definition a confusion matrix `C` is such that `C[i,j]` is equal to the number of observations known to be in group `i` but predicted to be in group `j`.\n",
    "\n",
    "and\n",
    "\n",
    "> Wikipedia and other references may use a different convention for axes.\n",
    "\n",
    "This means in the scikit learn convention the column-id is determined by the prediction.  This further means: as a visual point the horizontal position of cells in the scikit learn confusion matrix is determined by the prediction because matrices have the odd convention that the first index is row which specifies what vertical level one is referring to.\n",
    "\n",
    "Frankly we think scikit learn has the right rendering choice: consistency and legibility over convention. As Nina Zumel [demonstrated](http://www.win-vector.com/blog/2019/09/why-do-we-plot-predictions-on-the-x-axis/): there are good reasons to have predictions on the x-axis for plots, and the same holds for diagrams or matrices.\n",
    "\n",
    "So let's look at this confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  27,   0],\n",
       "       [  3, 918,   4],\n",
       "       [  0,  23,  11]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(\n",
    "    y_true=df.y, \n",
    "    y_pred=df.prediction, \n",
    "    labels=['short_opportunity', 'indeterminate', 'long_opportunity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our claim is: the prediction is controlling left/right in this matrix and the actual value to be predicted is determining up/down.\n",
    "\n",
    "What we have noticed often in practice is: for unbalanced classification problems, there is more vertical than horizontal dispersion in such confusion matrices.  This means: the predictions tend to have less range than seen in the training data.  Though this is not always the case (especially when classes are closer to balanced), some counter examples please see [here](https://github.com/WinVector/pyvtreat/blob/master/Examples/Multinomial/MultinomialExample.md) and [here](https://github.com/WinVector/vtreat/blob/master/Examples/Multinomial/MultinomialExample.md).\n",
    "\n",
    "We can confirm this as we see there are 75 actual values of `y` that are not `intermediate` and only 32 values of `prediction` that are not intermediate.  As the rows of the confusion matrix match the `y`-totals and the columns of the confusion matrix match the `prediction` totals we can confirm the matrix is oriented as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['y']!='indeterminate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['prediction']!='indeterminate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right or wrong, the model only identifies about one half the rate of possible extreme situations. This is not a pathology, but a typical conservative failure: good models tend to have less variation than their training data (or not more than, especially when using regularized methods).  I would try to liken this to the [regression to mediocrity](https://en.wikipedia.org/wiki/Regression_toward_the_mean) effects [Nina Zumel already described clearly](http://www.win-vector.com/blog/2019/09/why-do-we-plot-predictions-on-the-x-axis/).\n",
    "\n",
    "Of course one can try to adjust the per-class thresholds to find more potential trading opportunities. However, in my experience the new opportunities found are often of lower quality than the ones initially identified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
